{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Internship .. Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task description\n",
    "\n",
    "In this task, we are `building a code evaluator` that can `take input` from the user as follows:\n",
    "\n",
    "- `Task description.`\n",
    "\n",
    "- `Code (solution).`\n",
    "\n",
    "The `output` should be the following:\n",
    "\n",
    "-`Is this code solving the task description`? And if not, how close is this code to the task description?\n",
    "\n",
    "-Is this code `modularized`?\n",
    "\n",
    "-How efficient is this code from the `performance`, `clean code`, and `readability` perspective?\n",
    "\n",
    "-I need make sure that the code contains the main concepts like `Preprocessing`, `splitting the data`, `training the model`, `testing the model`, and any other section you see suitable.\n",
    "\n",
    "-`Ensure that the code is AI-generated`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prompt engineering\n",
    "\n",
    "ChatGPT prompt engineering is the practice of crafting prompts that effectively instruct ChatGPT to generate outputs. In order to accurately guide ChatGPT’s responses, you need to understand its behavior and adjust your wording accordingly.\n",
    "\n",
    "When engineering the best ChatGPT prompts, focus on these principles:\n",
    "\n",
    "Clarity: ChatGPT prompts should be unambiguous and precise. Avoid vague language in order to ensure the best results.\n",
    "\n",
    "Simplicity: Keep prompts short yet powerful. One to three sentences is the ideal length of a prompt.\n",
    "\n",
    "Context: Include as much contextual information as possible. For example, if your prompt is job-related, include the industry, job title and specific task or goal.\n",
    "\n",
    "Specificity: Include details and specifics to personalize the output. In the history student example, provide details like subject interests or core skills.\n",
    "\n",
    "Role-Playing: Use role-playing methods to set the scope of the AI model’s responses. \n",
    "For example, you might say: “Pretend you’re a Microsoft engineer, and I’m an interviewee for a junior software engineer role. Generate three questions you might ask when interviewing me.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import openai\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_key = \"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\"\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1:\n",
    "-`Is this code solving the task description`? And if not, how close is this code to the task description?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This code solves the task very well. It takes a list of numbers as input and returns the sum of all even numbers in the list. It is efficient and straightforward.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analyze_code(task_description, code_solution):\n",
    "    prompt = f\"Code Analysis: {task_description}\\n\\nCode Solution:\\n{code_solution}\\n\\nHow well does this code solve the task?\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=300,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "task_description = \"\"\"\n",
    "Write a function that takes a list of numbers as input and returns the sum of all even numbers in the list.\n",
    "\"\"\"\n",
    "code_solution = \"\"\"\n",
    "def sum_even_numbers(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        if num % 2 == 0:\n",
    "            total += num\n",
    "    return total\n",
    "\"\"\"\n",
    "analysis_result = analyze_code(task_description, code_solution)\n",
    "analysis_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"No, this code does not perfectly solve the task. For example, the code does not specify which type of model to use. Additionally, the code does not provide any information about how to optimize the model's hyperparameters or how to decide on the best model to use. \\n\\nI would give this code a 90% closeness to the task description. To improve it, I would suggest adding code for hyperparameter optimization and model selection.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assess_closeness(task_description, code_solution):\n",
    "    prompt = f\"How Well and How Close Code Ninja: {task_description}\\n\\nCode Solution:\\n{code_solution}\\n\\nDoes this code perfectly solve the task? If not, provide details. Additionally, what percentage of the code's closeness to the task description would you give? If the percentage is less than 100%, please provide feedback or suggestions on how to improve it.\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=500,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "task_description = \"\"\"\n",
    "Build a machine learning model to classify images of cats and dogs. The dataset consists of 1000 images of cats and 1000 images of dogs. The goal is to preprocess the data, split it into training and testing sets, train a model using the training data, and evaluate the model's performance on the testing data.\n",
    "\"\"\"\n",
    "code_solution = \"\"\"\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Testing the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "preprocessed_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\"\"\"\n",
    "closeness_result = assess_closeness(task_description, code_solution)\n",
    "closeness_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2:\n",
    "-Is this code `modularized`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This code is very well modularized. Each task has been broken down into separate functions and libraries, making the code easier to read and debug. The code is also well organized and clearly commented, making it easier to find specific sections of the code. Additionally, the code is efficient and does not repeat any tasks unnecessarily.'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assess_modularity(code_solution):\n",
    "    # prompt = f\"Modularity Assessment:\\n{code_solution}\\n\\nIs the code modularized? How well?\"\n",
    "    prompt = f\"Code Modularity Masterpiece:\\n{code_solution}\\n\\nHow well is this code modularized? Provide insights into the modularity of the code.\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "code_solution = \"\"\"\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Testing the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "preprocessed_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\"\"\"\n",
    "result = assess_modularity(code_solution)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3:\n",
    "-How efficient is this code from the `performance`, `clean code`, and `readability` perspective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This code is efficient and has good performance. It is written in a concise and clear manner, adhering to coding standards and best practices. It is also well-structured and easy to read, making it easy to understand and maintain.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assess_efficiency(code_solution):\n",
    "    prompt = f\"Code Efficient Analysis:\\n{code_solution}\\n\\nAssess the code in terms of performance, clean code, and readability. For performance: consider efficiency and speed. For clean code: focus on simplicity, clarity, and adherence to coding standards. For readability: think about how easily someone else could understand and maintain this code ninja.\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=500,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "code_solution = \"\"\"\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Testing the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "preprocessed_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\"\"\"\n",
    "result = assess_efficiency(code_solution)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4:\n",
    "-I need make sure that the code contains the main concepts like `Preprocessing`, `splitting the data`, `training the model`, `testing the model`, and any other section you see suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided code covers all the fundamental concepts and is complete. It includes preprocessing, data splitting, model training, model testing, and evaluation sections.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_main_concepts(code_solution):\n",
    "    prompt = f\"Code Mastery Validation:\\n{code_solution}\\n\\nEvaluate the provided code to ensure it encompasses the following main concepts:\\n\\n1. **Preprocessing:** Look for steps or functions related to data preprocessing, such as cleaning, normalization, or feature engineering.\\n\\n2. **Data Splitting:** Check for code segments that split the data into training and testing sets or employ techniques like cross-validation.\\n\\n3. **Model Training:** Identify sections that involve training a machine learning model, including the selection of algorithms and hyperparameter tuning.\\n\\n4. **Model Testing:** Examine the code for steps involving the testing and evaluation of the trained model on unseen data.\\n\\n5. **Other Relevant Sections:** Verify the presence of any additional sections relevant to the task or specific requirements.\\n\\nAssess the completeness of the code in covering these fundamental aspects.\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=700, \n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "code_solution = \"\"\"\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Testing the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "preprocessed_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\"\"\"\n",
    "result = validate_main_concepts(code_solution)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove step preprocessing and test data and test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided code covers all the main concepts of a machine learning project. It preprocesses the data by loading it into a dataset, splits the data into training and testing sets, trains the model using the SVC algorithm, and evaluates the model on unseen data. Additionally, it also includes a random_state parameter to ensure consistent results. Therefore, the code is complete and covers all the fundamental aspects of a machine learning project.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_main_concepts(code_solution):\n",
    "    prompt = f\"Code Mastery Validation:\\n{code_solution}\\n\\nEvaluate the provided code to ensure it encompasses the following main concepts:\\n\\n1. **Preprocessing:** Look for steps or functions related to data preprocessing, such as cleaning, normalization, or feature engineering.\\n\\n2. **Data Splitting:** Check for code segments that split the data into training and testing sets or employ techniques like cross-validation.\\n\\n3. **Model Training:** Identify sections that involve training a machine learning model, including the selection of algorithms and hyperparameter tuning.\\n\\n4. **Model Testing:** Examine the code for steps involving the testing and evaluation of the trained model on unseen data.\\n\\n5. **Other Relevant Sections:** Verify the presence of any additional sections relevant to the task or specific requirements.\\n\\nAssess the completeness of the code in covering these fundamental aspects.\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=700,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "code_solution = \"\"\"\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "result = validate_main_concepts(code_solution)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "umm still responses not good enough! so change improve prompt again and try again with remove step preprocessing and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_main_concepts(code_solution):\n",
    "    prompt = f\"Code Mastery Validation:\\n{code_solution}\\n\\nEvaluate the provided code to ensure it encompasses the following main concepts:\\n\\n1. **Preprocessing:** Examine the code for steps or functions related to data preprocessing, such as cleaning, normalization, or feature engineering. If preprocessing is present, describe the techniques used. If not, provide feedback on the absence of preprocessing and suggest relevant techniques to enhance the robustness of the machine learning model.\\n\\n2. **Data Splitting:** Check for code segments that split the data into training and testing sets or employ techniques like cross-validation. If data splitting is present, describe the methods used. If not, provide feedback on the absence of data splitting and suggest appropriate techniques.\\n\\n3. **Model Training:** Identify sections that involve training a machine learning model, including the selection of algorithms and hyperparameter tuning. If model training is present, describe the algorithms and hyperparameter tuning methods used. If not, provide feedback on the absence of model training and suggest suitable algorithms.\\n\\n4. **Model Testing:** Examine the code for steps involving the testing and evaluation of the trained model on unseen data. If model testing is present, describe the evaluation metrics used. If not, provide feedback on the absence of model testing and suggest relevant evaluation metrics.\\n\\n5. **Other Relevant Sections:** Verify the presence of any additional sections relevant to the task or specific requirements. Provide details if found, and offer feedback if any relevant sections are missing.\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=1200,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "\n",
    "code_solution = \"\"\"\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "result = validate_main_concepts(code_solution)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prompt so long so not return anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary prompt and try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The provided code is missing some essential steps, such as preprocessing the data, performing hyperparameter tuning, and evaluating the model's performance. \\n\\nFor preprocessing, it is important to make sure that the data is properly formatted and normalized. This may involve scaling the data, or converting categorical data into numerical data.\\n\\nTo perform hyperparameter tuning, it is important to use techniques such as cross-validation or grid search to find the optimal parameters for the model.\\n\\nFinally, to evaluate the model's performance, it is important to use appropriate metrics such as precision, recall, and accuracy.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_main_concepts(code_solution):\n",
    "    prompt = f\"code_solution:\\n{code_solution}\\n\\nEnsure that the provided code encompasses the main concepts required, including 'Preprocessing, splitting the data, training the model and testing the model'. Provide feedback if any of these concepts are absent, and suggest appropriate techniques to enhance the robustness of this code.\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=1200,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "code_solution = \"\"\"\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "result = validate_main_concepts(code_solution)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i think response is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 5:\n",
    "-`Ensure that the code is AI-generated`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This code appears to be manually written.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensure_ai_generated(code_solution):\n",
    "    prompt = f\"Code Origin Verification:\\n{code_solution}\\n\\nVerify if the provided code is AI-generated if yes Confirm its authenticity and acknowledge. If code appears manually written, provide appropriate feedback.\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=300,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example usage\n",
    "code_solution = \"\"\"\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Testing the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "preprocessed_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, predictions)\"\"\"\n",
    "\n",
    "result = ensure_ai_generated(code_solution)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "improve prompt again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This code appears to be manually written and not AI-generated. There are no suspicious elements, but it should be noted that the code does not include any measures to prevent overfitting or other techniques that are typically used in machine learning.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensure_ai_generated(code_solution):\n",
    "    prompt = f\"Code Origin Verification:\\n{code_solution}\\n\\nVerify if the provided code is AI-generated if yes Confirm its authenticity and acknowledge. If code appears manually written, provide appropriate feedback on any suspicious elements or reasons for the manual appearance..\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=300,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example usage\n",
    "code_solution = \"\"\"\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Testing the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "preprocessed_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\"\"\"\n",
    "\n",
    "result = ensure_ai_generated(code_solution)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col1, col2 = st.columns(2)\n",
    "# task_description = col1.text_area(\"Task Description:\")\n",
    "# code_solution = col2.text_area(\"Code Solution:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    task_description = \"Build a machine learning model to classify images of cats and dogs. The dataset consists of 1000 images of cats and 1000 images of dogs. The goal is to preprocess the data, split it into training and testing sets, train a model using the training data, and evaluate the model's performance on the testing data.\"\n",
    "    code_solution = \"\"\"\n",
    "    # Preprocessing\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Splitting the data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Training the model\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Testing the model\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = load_dataset()\n",
    "\n",
    "    # Preprocess the data\n",
    "    scaler = StandardScaler()\n",
    "    preprocessed_data = scaler.fit_transform(dataset)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(preprocessed_data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create and train the model\n",
    "    model = SVC()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the testing data\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model's performance\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \"\"\"\n",
    "\n",
    "    closeness_result = assess_closeness(task_description, code_solution)\n",
    "    time.sleep(10) # To handel api key, not to run code more than 3 times in 20 seconds.\n",
    "    modularity_result = assess_modularity(code_solution)\n",
    "    time.sleep(10)\n",
    "    quality_result = assess_efficiency(code_solution)\n",
    "    time.sleep(10)\n",
    "    validation_result = validate_main_concepts(code_solution)\n",
    "    time.sleep(10)\n",
    "    generation_ai_result = ensure_ai_generated(code_solution)\n",
    "    \n",
    "    print(f\"**Closeness Assessment:**\\n\\n{closeness_result}\")\n",
    "    print(f\"**Modularity Assessment:**\\n\\n{modularity_result}\")\n",
    "    print(f\"**Code Quality Assessment:**\\n\\n{quality_result}\")\n",
    "    print(f\"**Main Concepts Validation:**\\n\\n{validation_result}\")\n",
    "    print(f\"**Generation AI Check:**\\n\\n{generation_ai_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Closeness Assessment:**\n",
      "\n",
      "No, this code does not perfectly solve the task, as it does not include code for loading the dataset or for setting the labels. Additionally, the percentage of the code's closeness to the task description would be around 85%, as the code includes the necessary steps but is missing some important parts. To improve it, the code should include code for loading the dataset and setting the labels, and should add code for hyperparameter tuning to optimize the model's performance.\n",
      "**Modularity Assessment:**\n",
      "\n",
      "This code is very well modularized. The code is broken down into smaller, more manageable chunks that are easy to read and understand. The code is organized in a way that each task is separated into its own section, allowing for easy debugging and modification. Additionally, there is a clear separation between the data preprocessing, training, testing, and evaluation stages, which makes the code more organized and easier to follow.\n",
      "**Code Quality Assessment:**\n",
      "\n",
      "The code is efficient and reasonably optimized for performance. It uses pre-existing libraries for preprocessing, splitting, training, and testing which reduces time and effort. It also uses a random number generator to ensure that the train and test sets are randomly chosen. \n",
      "\n",
      "The code is clean and well-structured, making it easy to read and understand. It follows coding standards and is written in a structured and organized manner. The variables are named in a way that makes it easy to understand their purpose. \n",
      "\n",
      "The code is highly readable, making it easy for someone else to understand and maintain. The comments provide useful context and explain the purpose of each step in the process. The code is also organized into logical sections and the variable names make it easy to understand the purpose of each section.\n",
      "**Main Concepts Validation:**\n",
      "\n",
      "The provided code is a good starting point for training and testing a SVM model. However, there are some additional steps that can be taken to improve the robustness of the code. \n",
      "\n",
      "First, it is important to include steps for performing cross validation on the training data. This will allow us to ensure that the model is not overfitting, and that it is generalizing well to unseen data. This can be done by using the KFold class from the sklearn.model_selection library.\n",
      "\n",
      "Second, it is important to include a step for parameter tuning. This can be done using GridSearchCV from the sklearn.model_selection library, which allows us to systematically search for the best hyperparameters for the model.\n",
      "\n",
      "Finally, it is important to include a step for evaluating the model's performance. This can be done by calculating metrics such as precision, recall, and f1-score in addition to accuracy. This can be done using the classification_report method from the sklearn.metrics library.\n",
      "**Generation AI Check:**\n",
      "\n",
      "This code appears to be manually written. There are no suspicious elements, however it should be reviewed to ensure that it is performing the intended functions properly.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remember that was error in case 4 which was prompt is so long, i will handel this error.\n",
    "\n",
    "I think we should pass code as parts not full code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error: Unable to retrieve a valid response from the API, prompt is so long, please pass the code in parts.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_main_concepts(code_solution):\n",
    "    prompt = f\"Code Mastery Validation:\\n{code_solution}\\n\\nEvaluate the provided code to ensure it encompasses the following main concepts:\\n\\n1. **Preprocessing:** Examine the code for steps or functions related to data preprocessing, such as cleaning, normalization, or feature engineering. If preprocessing is present, describe the techniques used. If not, provide feedback on the absence of preprocessing and suggest relevant techniques to enhance the robustness of the machine learning model.\\n\\n2. **Data Splitting:** Check for code segments that split the data into training and testing sets or employ techniques like cross-validation. If data splitting is present, describe the methods used. If not, provide feedback on the absence of data splitting and suggest appropriate techniques.\\n\\n3. **Model Training:** Identify sections that involve training a machine learning model, including the selection of algorithms and hyperparameter tuning. If model training is present, describe the algorithms and hyperparameter tuning methods used. If not, provide feedback on the absence of model training and suggest suitable algorithms.\\n\\n4. **Model Testing:** Examine the code for steps involving the testing and evaluation of the trained model on unseen data. If model testing is present, describe the evaluation metrics used. If not, provide feedback on the absence of model testing and suggest relevant evaluation metrics.\\n\\n5. **Other Relevant Sections:** Verify the presence of any additional sections relevant to the task or specific requirements. Provide details if found, and offer feedback if any relevant sections are missing.\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=1200,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    response = response.choices[0].text.strip() if response.choices else None\n",
    "    return response if response else \"Error: Unable to retrieve a valid response from the API, prompt is so long, please pass the code in parts.\"\n",
    "\n",
    "\n",
    "code_solution = \"\"\"\n",
    "# Splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Training the model\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\"\"\"\n",
    "result = validate_main_concepts(code_solution)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
